

function mcmc(x, Œ∏, proposals, n; every=10)
    @unpack ps, R, prior = Œ∏
    L = length(ps)
    # row i contains the data to compute the gff at locus i
    # col j contains the contribution of locus j to all locus-specific gffs
    M = [i == j ? 0.0 : -x[j] * ps[j] / R[i,j] for i=1:L, j=1:L]
    ‚Ñì = ‚Ñìhood(M, x, Œ∏)
    œÄ = sum(logpdf.(prior, x))
    x_ = copy(x)
    M_ = copy(M)
    X  = Matrix{Float64}(undef, n, L)
    ‚Ñìs = Vector{Float64}(undef, n)
    for k=1:n
        k % every == 0 && @info k, ‚Ñì + œÄ 
        for i=1:L
            x, M, ‚Ñì, œÄ = update!(i, x_, x, M_, M, ‚Ñì, œÄ, Œ∏, proposals[i])
        end
        X[k,:] .= x
       ‚Ñìs[k]   = ‚Ñì + œÄ
    end
    return X, ‚Ñìs
end

function update!(i, x_, x, M_, M, ‚Ñì, œÄ, Œ∏, prop)
    # update selection coefficient at locus i 
    x_[i], q = prop(x[i])
    M_[:,i] .= M[:,i] .* (x_[i]/x[i])
    # compute the likelihood, in principle this requires recomputing all
    ‚Ñì_ = ‚Ñìhood(M_, x_, Œ∏)
    œÄ_ = œÄ + logpdf(Œ∏.prior, x_[i]) - logpdf(Œ∏.prior, x[i])
    if log(rand()) <  ‚Ñì_ + œÄ_ - ‚Ñì - œÄ + q
        ‚Ñì = ‚Ñì_
        œÄ = œÄ_
        x[i] = x_[i]
        M[:,i] .= M_[:,i]
        accept!(prop)
    end
    return x, M, ‚Ñì, œÄ
end

# Do we need `Wright`? Or can we get away with the unnormalized pdf? I guess not:
# the normalization constant is only constant wrt to p, it is a function of the
# parameters...
function ‚Ñìhood(M, x, Œ∏)
    @unpack ps, m, N, R, u = Œ∏
    me = m*exp.(vec(sum(M, dims=2)))
    ds = Wright.(Ref(N), Ref(u), u .+ me, -x, Ref(0.0))
    ‚Ñìs = logpdf.(ds, ps)
    return sum(‚Ñìs) 
end

function soften(ps)
    œµ = minimum(filter(x->x!=0.0, ps))/10
    [p == 0.0 ? œµ : p == 1 ? 1-œµ : p for p in ps]   
end



# The same, but update a number of loci simultaneously
# ========================================================================
function mcmc1(x, Œ∏, proposals, n; every=10)
    @unpack ps, R, prior, q = Œ∏
    L = length(ps)
    # row i contains the data to compute the gff at locus i
    # col j contains the contribution of locus j to all locus-specific gffs
    M = [i == j ? 0.0 : -x[j] * ps[j] / R[i,j] for i=1:L, j=1:L]
    ‚Ñì = ‚Ñìhood(M, x, Œ∏)
    œÄ = sum(logpdf.(prior, x))
    x_ = copy(x)
    M_ = copy(M)
    X  = Matrix{Float64}(undef, n, L)
    ‚Ñìs = Vector{Float64}(undef, n)
    for k=1:n
        k % every == 0 && @info k, ‚Ñì + œÄ 
        #K = min(L, rand(Geometric(q)+1))
        idxs = shuffle_partition(L, q)
        for idx in idxs
            x, M, ‚Ñì, œÄ = update1!(idx, x_, x, M_, M, ‚Ñì, œÄ, Œ∏, proposals)
        end
        X[k,:] .= x
        ‚Ñìs[k]   = ‚Ñì + œÄ
    end
    return X, ‚Ñìs
end

shuffle_partition(N, k) = (collect ‚àò partition)(shuffle(1:N), k)

function update1!(idx, x_, x, M_, M, ‚Ñì, œÄ, Œ∏, proposals)
    # update selection coefficient at locus i 
    œÄ_ = œÄ 
    q_ = 0.0
    for i in idx
        x_[i], q = proposals[i](x[i])
        M_[:,i] .= M[:,i] .* (x_[i]/x[i])
        œÄ_ += logpdf(Œ∏.prior, x_[i]) - logpdf(Œ∏.prior, x[i])
        q_ += q
    end
    # compute the likelihood, in principle this requires recomputing all
    ‚Ñì_ = ‚Ñìhood(M_, x_, Œ∏)
    if log(rand()) <  ‚Ñì_ + œÄ_ - ‚Ñì - œÄ + q_
        ‚Ñì = ‚Ñì_
        œÄ = œÄ_
        for i in idx
            x[i] = x_[i]
            M[:,i] .= M_[:,i]
            accept!(proposals[i])
        end
    end
    return x, M, ‚Ñì, œÄ
end


# Approximate approach for DFE inference
# =========================================================================
# Would be good to have a category of neutral loci: one more parameter p0

function egff(ps, R, Œ±, Œ∏)
    L  = length(ps)
    ùîºg = zeros(L)
    for i=1:L
        lùîºgi = 0.0
        for j=1:L
            i == j && continue
            lùîºgi += log(1 + Œ∏*(ps[j]/R[i,j]))
        end
        ùîºg[i] = exp(-Œ± * lùîºgi)
    end
    return ùîºg
end

function discretize(d, K)
    qstart = 1.0/2K
    qend = 1. - 1.0/2K
    xs = quantile.(d, qstart:(1/K):qend)
    xs .* (mean(d)*K/sum(xs))  # rescale by factor mean(d)/mean(xs)
end

function ‚Ñìhood2(g, x, Œ∏)
    @unpack ps, m, N, R, u, K = Œ∏
    L  = length(ps)
    me = m*g
    ‚Ñì  = 0.0
    dfe = Gamma(x[1], x[2])
    ys  = discretize(dfe, K)
    for i=1:L
        ‚Ñìi = 0.0
        for k=1:K
            ‚Ñìi += pdf(Wright(N, u, u + me[i], -ys[k], 0.0), ps[i])
        end
        ‚Ñì += log(‚Ñìi) - log(K)
    end
    return ‚Ñì
end

function mcmc2(x, Œ∏, proposals, n; every=10)
    @unpack ps, R, prior = Œ∏
    g = egff(ps, R, x[1], x[2])
    ‚Ñì = ‚Ñìhood2(g, x, Œ∏)
    œÄ = logpdf(prior[1], x[1]) + logpdf(prior[2], x[2])
    x_ = copy(x)
    X  = Matrix{Float64}(undef, n, 2)
    ‚Ñìs = Vector{Float64}(undef, n)
    for k=1:n
        k % every == 0 && @printf "%6d %12.3f %9.3f %9.5f\n" k (‚Ñì + œÄ) x[1] x[2] 
        for i=1:2
            x, ‚Ñì, œÄ = update2!(i, x_, x, ‚Ñì, œÄ, Œ∏, proposals[i])
        end
        X[k,:] .= x
        ‚Ñìs[k]   = ‚Ñì + œÄ
    end
    return X, ‚Ñìs
end

function update2!(i, x_, x, ‚Ñì, œÄ, Œ∏, prop)
    @unpack ps, R, prior = Œ∏
    # update selection coefficient at locus i 
    x_[i], q = prop(x[i])
    # compute the likelihood, in principle this requires recomputing all
    g  = egff(ps, R, x_[1], x_[2])
    ‚Ñì_ = ‚Ñìhood2(g, x_, Œ∏)
    œÄ_ = logpdf(prior[1], x_[1]) + logpdf(prior[2], x_[2])
    if log(rand()) <  ‚Ñì_ + œÄ_ - ‚Ñì - œÄ + q
        ‚Ñì = ‚Ñì_
        œÄ = œÄ_
        x[i] = x_[i]
        accept!(prop)
    end
    return x, ‚Ñì, œÄ
end

# Under this approach, it would be useful to classify loci according the DFE
# model, or better, sample from the posterior of their selection coefficient
# given the posterior for the DFE model.

# How do we do this? Consider we assume the posterior mean DFE. For a given
# locus i, we calculate the Egff. Then we know the distribution of allele
# frequencies for this site given some s (likelihood model), and we can use
# this to sample s. The problem is one-dimensional, so we can calculate the
# posterior by integration.


function posts(p, dfe, gff, Œ∏; mins=1e-9, maxs=0.2)
    # P(s|p) = P(p|s)P(s)/P(p) = P(p|s)P(s)/‚à´P(p|s)P(s)ds
    # calculate the normalizing constant
    ‚Ñì(s) = pdf(Wright(Œ∏.N, Œ∏.u, Œ∏.u + Œ∏.m*gff, -s, 0.0), p)
    Z, _ = quadgk(s->‚Ñì(s)*pdf(dfe,s), mins, maxs)
    post(s) = ‚Ñì(s)*pdf(dfe,s) / Z
    return post 
end

function findquantile(f, q; xmin=1e-9, xmax=0.2)
    g(x) = quadgk(f, xmin, x)[1] - q
    find_zero(g, [xmin, xmax])
end

# with a class of neutral genes
# ===================================================================
# Could we resample which genes are neutral and which selected? It is something
# we would like to infer, and it need not necessarily make computation more
# tricky? Or does it? 
# P(selected=0|x) = P(x|selected=0)œÄ0/(Œ£‚Çõ P(x|selected=1,s)*P(s)*(1-œÄ0) + P(x|selected=0)*œÄ0)
function egff(ps, R, Œ±, Œ∏, œÄ0)
    L  = length(ps)
    ùîºg = zeros(L)
    for i=1:L
        lùîºgi = 0.0
        for j=1:L
            i == j && continue
            lùîºgi += log(œÄ0 + (1-œÄ0)*(1 + Œ∏*(ps[j]/R[i,j]))^(-Œ±))
        end
        ùîºg[i] = exp(lùîºgi)
    end
    return ùîºg
end

function discretize(d, K)
    qstart = 1.0/2K
    qend = 1. - 1.0/2K
    xs = quantile.(d, qstart:(1/K):qend)
    xs .* (mean(d)*K/sum(xs))  # rescale by factor mean(d)/mean(xs)
end

function ‚Ñìhood3(g, x, Œ∏; K=8)
    @unpack ps, m, N, R, u = Œ∏
    L  = length(ps)
    me = m*g
    ‚Ñì  = 0.0
    dfe = Gamma(x[1], x[2])
    œÄ0  = x[3]
    ys  = discretize(dfe, K)
    for i=1:L
        ‚Ñìi = 0.0
        for k=1:K
            ‚Ñìi += pdf(Wright(N, u, u + me[i], -ys[k], 0.0), ps[i])
        end
        ‚Ñìi *= (1-œÄ0)/K
        ‚Ñìi += œÄ0*pdf(Wright(N, u, u + me[i], 0.0, 0.0), ps[i]) 
        ‚Ñì  += log(‚Ñìi)
    end
    return ‚Ñì
end

function mcmc3(x, Œ∏, proposals, n; every=10)
    @unpack ps, R, prior = Œ∏
    g = egff(ps, R, x[1], x[2], x[3])
    ‚Ñì = ‚Ñìhood3(g, x, Œ∏)
    œÄ = logpdf(prior[1], x[1]) + logpdf(prior[2], x[2]) + logpdf(prior[3], x[3])
    x_ = copy(x)
    X  = Matrix{Float64}(undef, n, 3)
    ‚Ñìs = Vector{Float64}(undef, n)
    for k=1:n
        k % every == 0 && @printf "%6d %12.3f %9.3f %9.5f %9.3f\n" k (‚Ñì + œÄ) x[1] x[2] x[3] 
        for i=1:3
            x, ‚Ñì, œÄ = update3!(i, x_, x, ‚Ñì, œÄ, Œ∏, proposals[i])
        end
        X[k,:] .= x
        ‚Ñìs[k]   = ‚Ñì + œÄ
    end
    return X, ‚Ñìs
end

function update3!(i, x_, x, ‚Ñì, œÄ, Œ∏, prop)
    @unpack ps, R, prior = Œ∏
    # update selection coefficient at locus i 
    x_[i], q = prop(x[i])
    # compute the likelihood, in principle this requires recomputing all
    g  = egff(ps, R, x_[1], x_[2], x_[3])
    ‚Ñì_ = ‚Ñìhood3(g, x_, Œ∏)
    œÄ_ = logpdf(prior[1], x_[1]) + logpdf(prior[2], x_[2]) + logpdf(prior[3], x_[3])
    if log(rand()) <  ‚Ñì_ + œÄ_ - ‚Ñì - œÄ + q
        ‚Ñì = ‚Ñì_
        œÄ = œÄ_
        x[i] = x_[i]
        accept!(prop)
    end
    return x, ‚Ñì, œÄ
end
